####################################################################
## Snakefile Général : déroulement complet du workflow d'analyse :##
####################################################################

## Répartition du nombre de threads total

import sys, os
from math import *
argument_list = sys.argv[1:]
print(argument_list)
if '--cores' in argument_list:
	T = int(argument_list[argument_list.index('--cores') + 1])

elif '--force' in argument_list:
	T = int(argument_list[argument_list.index('--force') + 1][2:])

# répartition théori:que des threads pour les branches 1 et 2 du pipeline (download sra- makefastq, download chromosomes-index)

t_branch1 = ceil(T/2)
t_branch2 = ceil(T - t_branch1)

#1 paramètre de threads pour chaque règle le nécéssitant
t_sra = ceil(t_branch1/4)
t_makefastq = ceil(t_branch1/4)
t_index = ceil(t_branch2)
t_read_count = ceil(T/4)

## Définition des variables pour les règles suivantes :

ID = ["1", "2"]
SAMPLES=["SRR628582","SRR628583","SRR628584","SRR628585","SRR628586","SRR628587","SRR628588","SRR628589"]
KLIST = ["1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "MT", "X", "Y"]

## Report

report: "report/workflow.rst"


## Règle all :

rule all:
	input:
		report("fichier.Rdata", category = "final output")


###########################################################################################################################################
## Etape 1 : téléchargement et traitement des fichiers .sra pour obtenir des fichiers .fastq utilisables pour l'indexage et le mapping : ##
###########################################################################################################################################


# Téléchargement des fichiers .sra pour le début de l'analyse :
		
rule download_sra:
	output: 
		report("sraFiles/{sample}.1", category = "download SRA files")
	threads : t_sra
	shell:
		"""
		wget -O {output} https://sra-downloadb.be-md.ncbi.nlm.nih.gov/sos1/sra-pub-run-5/{wildcards.sample}/{wildcards.sample}.1
		"""

# Transformation des fichiers sra en .fastq :

rule makefastq:
	input:
		"sraFiles/{sample}.1"
	singularity: 
		"docker://evolbioinfo/sratoolkit:v2.10.8"
	output:
		report(expand("fastqFiles/{{sample}}.1_{exp}.fastq",exp = ["1","2"]), category = "SRA to FASTQ")
	shell: 
		"""
		fastq-dump --split-files {input} -O fastqFiles/

		"""

######################################################################################################################
## Etape 2 : obtention du génome indexé : téléchargement (fait avant) et traitement des fichiers du génome humain : ##
######################################################################################################################

# Téléchargement des choromosomes pour l'indexage du génome :

rule download_chromosomes:
	output:
		report("chromosomes/{chromosome}.fa.gz", category = "downloading chromosomes")
	threads: 1
	shell:
		"""
		wget -O {output} ftp://ftp.ensembl.org/pub/release-101/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.{wildcards.chromosome}.fa.gz
		"""

# concaténer les fichiers du génome correspondant aux chromosomes pour en obtenir un unique correspondant au génome humain entier :

rule concatChr:
	input:
		expand("chromosomes/{chromosome}.fa.gz", chromosome=KLIST)
	output:
		report("genome/genome.fa.gz", category = "Chromosomes concatenation")
	shell:
		"gunzip -c {input} | gzip -c > {output}"

# décompresser le fichier du génome entier obtenu :

rule gunzipGenome:
	input:
		"genome/genome.fa.gz"
	output:
		report("genome/genome.fa", category = "gunzip genome")
	shell:
		"""
		gunzip -c {input} > {output}
		"""

# indexer le génome obtenu :

rule index:
	input: 
		"genome/genome.fa"
	output:
		report("genome_index/genomeParameters.txt", category = "indexing genome")
	singularity: 
		"docker://evolbioinfo/star:v2.7.6a"
	threads : t_index
	shell:
		"""
		STAR --runThreadN {threads} --runMode genomeGenerate --genomeDir genome_index/ --genomeFastaFiles {input}
		"""

#########################################################
## Etape 3 : mapping des .fastq sur le génome indexé : ##
#########################################################


rule mapping:
	input:
		i1 = "fastqFiles/{sample}.1_1.fastq",
		i2 = "fastqFiles/{sample}.1_2.fastq",
		gparam = "genome_index/genomeParameters.txt"
	output:
		report("bamfiles/{sample}.bam", category = "mapping files")
	singularity:
		"docker://evolbioinfo/star:v2.7.6a"
	threads: T
	shell:
		"""
		STAR --outSAMstrandField intronMotif --outFilterMismatchNmax 4  --outFilterMultimapNmax 10 --genomeDir genome_index/ --readFilesIn {input.i1} {input.i2} --runThreadN {threads} --outSAMunmapped None --outSAMtype BAM SortedByCoordinate --outStd BAM_SortedByCoordinate --genomeLoad NoSharedMemory --limitBAMsortRAM 50000000000 > {output}
		"""

###################################
## Etape 4 : compter les reads : ##
###################################


# Télécharger un fichier d'annotations pour le comptage :


rule downloadGenomeAnnotation:
	output :
		report("file_genome_annotation/Homo_sapiens.GRCh38.101.chr.gtf", category = "download genome annotation file")
	run:
		shell("wget -P file_genome_annotation/ ftp://ftp.ensembl.org/pub/release-101/gtf/homo_sapiens/Homo_sapiens.GRCh38.101.chr.gtf.gz")
		shell("gunzip file_genome_annotation/Homo_sapiens.GRCh38.101.chr.gtf.gz")


# Comptage des reads et de leurs différences au génome de référence :

rule counting:
	input:
    		i1 = "bamfiles/{sample}.bam",
		gtf = "file_genome_annotation/Homo_sapiens.GRCh38.101.chr.gtf"
	output:
		report("countfiles/{sample}.counts", category = "counting reads")
	priority: 91
	singularity:
		"docker://evolbioinfo/subread:v2.0.1"
	threads: t_read_count
	shell:
		"""
		featureCounts -T {threads} -t gene -g gene_id -s 0 -a {input.gtf} -o {output} {input.i1}
		"""

####################################################
## Etape 5 : analyser les résultats de comptage : ##
####################################################

rule analyse_stat_R:
	input:
		expand("countfiles/{sample}.counts", sample=SAMPLES)
	output:
		report("fichier.Rdata", caption = os.path.join(workflow.basedir,"report/fichier.Rdata") category = "statistic analysis")
	singularity:
		"docker://evolbioinfo/deseq2:v1.28.1"
	script:
		"/home/ubuntu/gitrepository/hackathon_groupe6/script_R-analyse-stat.R"

